# Gesture-Controlled DJI Model ğŸšâœ‹

This project implements a **gesture-controlled system** to interact with a DJI drone using hand signs.  
It includes three main modules:
1. **Data Collection** â†’ `signDataset.py`  
2. **Model Training** â†’ `training.ipynb`  
3. **Live Detection** â†’ `liveDetect.py`

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Gestures/ # Folder containing collected gesture images
â”œâ”€â”€ pycache/ # Python cache files
â”œâ”€â”€ GestureModel.pt # Saved PyTorch model
â”œâ”€â”€ GestureModel.py # Model architecture definition
â”œâ”€â”€ best_gesture_model.pth # Best trained model weights
â”œâ”€â”€ liveDetect.py # Real-time gesture detection & control
â”œâ”€â”€ signDataset.py # Script for gesture dataset collection
â”œâ”€â”€ training.ipynb # Jupyter Notebook for training the model
â”œâ”€â”€ tempCodeRunnerFile.py # Temporary runner file (can be ignored)
â””â”€â”€ README.md # Project documentation
```

---

## ğŸš€ Features
- **Collect custom gesture datasets** using your webcam.  
- **Train deep learning models** for hand sign recognition.  
- **Perform real-time detection** and control a DJI drone.  

---

## âš™ï¸ Setup Instructions

1. **Clone the repository**
```bash
git clone https://github.com/your-username/Gesture-Controlled-DJI-Model.git
cd Gesture-Controlled-DJI-Model
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Connect your DJI Drone**  
Ensure your drone SDK/API is properly configured.  

---

## ğŸ“Š Workflow

### 1. Data Collection (`signDataset.py`)
Run the script to capture hand gesture images from your webcam.  
Data is saved in the `Gestures/` folder.  

```bash
python signDataset.py
```

---

### 2. Model Training (`training.ipynb`)
The model was trained using the collected gesture dataset.  
Training ran for ~60 epochs with the following results:

- **Final Training Accuracy:** ~99%  
- **Final Test Accuracy:** ~99%  
- **Loss steadily decreased** with no major signs of overfitting.  

#### Training Curves
Below are the training curves for **Loss** and **Accuracy**:

<img width="1222" height="624" alt="image" src="https://github.com/user-attachments/assets/655b7b2e-e82c-482e-8c59-e878d1fccf5b" />

---

### 3. Live Gesture Detection (`liveDetect.py`)
Run the live detection script to recognize gestures in real-time.  
The drone responds to recognized hand signs.  

```bash
python liveDetect.py
```

âœ… Example Gestures  

- ğŸ‘† â†’ Up / TakeOff  
- ğŸ‘ â†’ Down / Land  
- âœŠ â†’ Come Forward  
- âœ‹ â†’ Stop  
- ğŸ‘‰ â†’ Left  
- ğŸ«² â†’ Right  
- ğŸ¤˜ â†’ Turn Backward  

(Modify gesture mappings as needed in `liveDetect.py`)

---

## âš™ï¸ Steps to Implement Gesture Control on Tello

### 1. Install DJI Tello Python Library
```bash
pip install djitellopy
```

### 2. Connect to the Tello Drone
- Power on your Tello drone.  
- Connect your computer to Telloâ€™s Wi-Fi (SSID usually `TELLO-xxxxxx`).  
- Test connection with the following script:  

```python
from djitellopy import Tello

tello = Tello()
tello.connect()

print(f"Battery: {tello.get_battery()}%")
```

If everything is working, you should see the current battery percentage of the drone.  

---

### 3. Integrate with Your Gesture Detection
In `liveDetect.py`, after detecting a gesture, map it to drone commands.  

---

## ğŸ“Œ Future Improvements
- Add more robust gesture datasets.  
- Improve accuracy with transfer learning.  
- Extend support for additional drone commands.  

---

## ğŸ“ License
This project is licensed under the MIT License - feel free to use and modify.  

---

## ğŸ¤ Contributing
Pull requests are welcome! For major changes, open an issue first to discuss what youâ€™d like to change.  
